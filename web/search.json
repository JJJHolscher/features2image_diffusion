[
  {
    "objectID": "exploratory data analysis.html",
    "href": "exploratory data analysis.html",
    "title": "",
    "section": "",
    "text": ":wasm feature-images\n:dep wasm-bindgen = \"*\"\n:dep features2image_diffusion-widget = { path = \"../widget\" }\n:dep web-sys = { version = \"*\", features = [\"HtmlElement\"] }\nuse wasm_bindgen::prelude::*;\nuse features2image_diffusion_widget;\nuse web_sys::HtmlElement;\n\n\n\n\n\n\n\n:dep rand = \"*\"\nuse rand::distributions::{Alphanumeric, DistString};\n\nfn display_js(js: &str) {\n    let id = Alphanumeric.sample_string(&mut rand::thread_rng(), 16);\n    println!(\"EVCXR_BEGIN_CONTENT text/html\\n\n    &lt;link rel='stylesheet' href='../web/tailwind.css'&gt;\n    &lt;div id='{id}'&gt;&lt;/div&gt;\n    &lt;script&gt;\n        async function __evcxr_display() {{\n            evcxr = window.evcxr;\n            cwd = window.evcxr_cwd;\n            root = document.getElementById('{id}');\n            {js}\n        }};\n        if (window.evcxr.keys) {{\n            console.log('already exists');\n            __evcxr_display();\n        }} else {{\n            console.log('adding listener');\n            window.addEventListener('load', __evcxr_display);\n        }}\n    &lt;/script&gt;\n    \\nEVCXR_END_CONTENT\");\n}\n\n\n:dep features2image_diffusion-read-result = { path = \"../read-result\" }\nuse features2image_diffusion_read_result::create_file_tensor;\nuse features2image_diffusion_read_result::f2id_fs::Files;\nuse features2image_diffusion_read_result::list_paths::list_paths;\n\nlet RUN_ID = \"4409b6282a7d05f0b08880228d6d6564011fa40be412073ff05aff8bf2dc49fa\";\nlet RUN_DIR = format!(\"../xyz/run/{RUN_ID}\");\n\n\nlet files: Files = create_file_tensor(&RUN_DIR).unwrap();\nstd::fs::write(\"./files.bytes\", files.to_bytes()?)?;\nfiles.dfmt.shape()\n\n[18, 256, 4, 3]\n\n\n\ndisplay_js(&format!(\"\nresp = await fetch(cwd + 'files.bytes');\nevcxr.feature_images(\n    root,\n    new Uint8Array(await resp.arrayBuffer()),\n    cwd + '{RUN_DIR}'\n);\n\"));"
  },
  {
    "objectID": "image-gen-on-edited-features.html",
    "href": "image-gen-on-edited-features.html",
    "title": "",
    "section": "",
    "text": "%cd ..\n\n/home/jo3/p/features2image_diffusion\n\n\n/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n\n\n\nimport argtoml\nimport jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom pathlib import Path\nfrom typing import Tuple\n\nfrom jaxtyping import Float\nfrom jo3mnist.vis import to_img\nfrom torch.utils.data import DataLoader\n\nfrom features2image_diffusion.__main__ import evaluate\nfrom features2image_diffusion.data import load_mnist_with_features\nfrom features2image_diffusion.unet import load_ddpm\n\nnp.set_printoptions(precision=3)\n\nRUN_PATH = Path.cwd() / \"run/7927ad448cfdb96d1e819f599d5ce8a200a3de67068866289c8cf1e0215ec398\"\nO = argtoml.parse_args(toml_path=RUN_PATH / \"config.toml\")\nRUN = O.run[0]\nDEVICE = \"cuda\"\n\nTRAIN_SET, TEST_SET = load_mnist_with_features(\n    RUN.feature_dir, O.mnist_dir, RUN.batch_size, shuffle=False\n)\n\n/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/jo3util/warning.py:18: ToDoWarning: 'Make sure file names are numeric before the .npy.'\n  warnings.warn(msg, ToDoWarning)\n\n\n\ndef all_features_and_labels(dataset: DataLoader) -&gt; Tuple[\n    Float[torch.Tensor, \"datapoints features\"], \n    Float[torch.Tensor, \"datapoints\"]\n]:\n    \"\"\" Gather all features and labels into 2 big numpy arrays.\"\"\"\n    all_features = []\n    all_labels = []\n    for features, _, labels in dataset:\n        all_features.append(torch.Tensor(features))\n        all_labels.append(torch.Tensor(labels))\n    return torch.concatenate(all_features), torch.concatenate(all_labels)\n\nTRAIN_FEATURES, TRAIN_LABELS = all_features_and_labels(TRAIN_SET)\nprint(TRAIN_FEATURES.shape, TRAIN_LABELS.shape) \n\ntorch.Size([60000, 256]) torch.Size([60000])\n\n\n\n#X = np.log(TRAIN_FEATURES + 1e-20)\nTRAIN_FEATURES_AVG = torch.mean(TRAIN_FEATURES, axis=0)\nTRAIN_FEATURES_STD = torch.std(TRAIN_FEATURES, axis=0)\nplt.scatter(\n#plt.errorbar(\n    x=np.arange(TRAIN_FEATURES.shape[1]),\n    y=TRAIN_FEATURES_AVG,\n#    yerr=TRAIN_FEATURES_STD\n)\n\n\n\n\n\n\n\n\nThis does not seem very sparse. Let’s see how sparse an average datapoint is.\n\n@jax.vmap\ndef sparsity(point, thresh=0.01):\n    return np.sum(point &gt; thresh) / len(point)\n\nSPARSITY = sparsity(np.array(TRAIN_FEATURES))\nprint(np.mean(SPARSITY).item(), np.std(SPARSITY).item())\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\n0.5737580060958862 0.0127316415309906\n\n\nMore than half the features are active for any picture. That’s not good, we’ll need to run this again sometime. Let’s sort them by variance so we can guide our edits.\n\nFEATURES_SORTED_BY_STD = np.argsort(TRAIN_FEATURES_STD, axis=0)\nprint(FEATURES_SORTED_BY_STD[-10:])\nprint(TRAIN_FEATURES_AVG[FEATURES_SORTED_BY_STD][-10:])\nprint(TRAIN_FEATURES_STD[FEATURES_SORTED_BY_STD][-10:])\n\ntensor([ 60, 190, 150, 180, 251,  96, 215, 142, 176, 128])\ntensor([3.3336, 2.9315, 2.8921, 2.6445, 2.9475, 3.9932, 2.7286, 2.6410, 3.2082,\n        2.9412])\ntensor([1.5774, 1.5966, 1.6343, 1.6751, 1.7339, 1.7395, 1.7496, 1.7693, 1.7746,\n        1.7810])\n\n\nLet’s now get an example image.\n\ndel TRAIN_FEATURES\ndel TRAIN_LABELS\n\nEXAMPLE = next(iter(TRAIN_SET))\nEXAMPLE_FEATURES = torch.unsqueeze(EXAMPLE[0][0], 0)\nEXAMPLE_IMAGE = EXAMPLE[1][0]\nEXAMPLE_LABEL = EXAMPLE[2][0]\n\nprint(\"class =\", EXAMPLE_LABEL)\nto_img(EXAMPLE_IMAGE)\n\nclass = tensor(5)\n\n\n\n\n\n\n\n\n\nAnd now we let the ddpm generate variations on this image by conditioning it on the image’s features.\n\nDDPM = load_ddpm(\n    RUN_PATH / \"model/epoch-22.pth\",\n    EXAMPLE_FEATURES.shape[-1],\n    n_T=400,\n    device=DEVICE,\n    opts=RUN\n).eval()\n\n\ndef generate_and_show(\n    ddpm,\n    features,\n    num_generations,\n    original_img=None,\n    device=DEVICE\n):\n    # Generate images.\n    with torch.no_grad():\n        generations, _ = DDPM.sample(\n            features,\n            num_generations,\n            original_img.shape,\n            device\n        )\n    generations = generations.cpu().numpy()\n\n    # Determine the shape of the table of images.\n    num_images = num_generations\n    if original_img is not None:\n        num_images += 1\n    rows = int(np.sqrt(num_images))\n    cols = num_images // rows\n    if rows * cols &lt; num_images:\n        cols += 1\n    fig, axs = plt.subplots(rows, cols, facecolor=\"gray\")\n\n    # Place the images in the table.\n    for i, generation in enumerate(generations):\n        r = i // cols\n        c = i % cols\n        axs[r, c].imshow(to_img(generation))\n        axs[r, c].set_axis_off()\n    if original_img is not None:\n        axs[-1, -1].imshow(to_img(original_img))\n        axs[-1, -1].set_axis_off()\n    plt.show()\n\n    return generations\n\n_ = generate_and_show(DDPM, EXAMPLE_FEATURES, 5, EXAMPLE_IMAGE)\n\n\nsampling timestep 100\n\n\n\n\n\n\n\n\n\nThose are some crappy generations.\nLet’s see how they change as we change the features. The 128th features had the highest standard deviation, so let’s what it is here, and then change it.\n\nprint(EXAMPLE_FEATURES[0, 128])  # That's close to the average.\n\ndef edit_features(features, value, *index):\n    new_features = features.clone().detach()\n    slice = new_features\n    for i in index[:-1]:\n        slice = slice[i]\n    slice[index[-1]] = value\n    return new_features\n\nEDITED_FEATURES = edit_features(EXAMPLE_FEATURES, 0.0, 0, 128)\n_ = generate_and_show(DDPM, EDITED_FEATURES, 5, EXAMPLE_IMAGE)\n\ntensor(2.8017)\n\nsampling timestep 100"
  },
  {
    "objectID": "exploratory data analysis-Copy1.html",
    "href": "exploratory data analysis-Copy1.html",
    "title": "",
    "section": "",
    "text": "%cd ..\n\n/home/jo3/p/features2image_diffusion\n\n\n/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning:\n\nusing dhist requires you to install the `pickleshare` library.\n\n\n\n\nimport argtoml\nimport jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom pathlib import Path\nfrom typing import Tuple\n\nfrom jaxtyping import Float\nfrom jo3mnist.vis import to_img\nfrom torch.utils.data import DataLoader\n\nfrom features2image_diffusion.__main__ import evaluate\nfrom features2image_diffusion.data import load_mnist_with_features, loader_to_dataframe\nfrom features2image_diffusion.unet import load_ddpm\n\nnp.set_printoptions(precision=3)\n\nRUN_PATH = Path.cwd() / \"run/7927ad448cfdb96d1e819f599d5ce8a200a3de67068866289c8cf1e0215ec398\"\nO = argtoml.parse_args(toml_path=[RUN_PATH / \"config.toml\"])\nRUN = O[\"run\"][0]\nDEVICE = \"cuda\"\n\nprint(\"create the dataloader\")\n# TRAIN_LOADER, TEST_LOADER = load_mnist_with_features(\n    # RUN[\"feature_dir\"], O[\"mnist_dir\"], RUN[\"batch_size\"], shuffle=False\n# )\nprint(\"convert the loader to a dataframe\")\n# TRAIN_SET = loader_to_dataframe(TRAIN_LOADER, img_dir=\"./res/mnist/img\")\n\n\ncreate the dataloader\nconvert the loader to a dataframe\nLaunching the widget server\n\n\n(&lt;Thread(http server, started 129623161571008)&gt;,\n '&lt;!doctype html&gt;\\n&lt;html lang=\"en\"&gt;\\n  &lt;head&gt;\\n    &lt;script type=\"module\"&gt;\\n      import init, { COMPONENT } from \"/pkg/features2image_diffusion_dioxus_widgets.js\";\\n      async function main() {\\n        await init();\\n        COMPONENT(document.getElementById(\"dioxus-component\"));\\n      }\\n      main();\\n    &lt;/script&gt;\\n&lt;link rel=\"stylesheet\" href=\"/pkg/style.css\"&gt;\\n  &lt;/head&gt;\\n  &lt;body&gt;\\n    &lt;div id=\"dioxus-component\"ATTRIBUTES&gt;DATA&lt;/div&gt;\\n  &lt;/body&gt;\\n&lt;/html&gt;\\n')\n\n\n\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nRUN_ID = \"4409b6282a7d05f0b08880228d6d6564011fa40be412073ff05aff8bf2dc49fa\"\n\ndef get_img(run_id, data_id, feature_id, mod_id):\n    \n    if mod_id == -1:\n        path = Path.cwd() / \"run\" / RUN_ID / str(data_id) / \"unedited-images.png\"\n    else:\n        path = Path.cwd() / \"run\" / RUN_ID / str(data_id) / str(feature_id) / f\"{mod_id}-images.png\"\n\n    return mpimg.imread(path)\n        \nplt.imshow(get_img(RUN_ID, 0, 0, -1))\nplt.show()\n\n\n\n\n\n\n\n\n----------------------------------------\nException occurred during processing of request from ('127.0.0.1', 56616)\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib/python3.11/socketserver.py\", line 348, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib/python3.11/socketserver.py\", line 361, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/dioxus_widget/__init__.py\", line 226, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/dioxus_widget/__init__.py\", line 206, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/usr/lib/python3.11/http/server.py\", line 671, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/usr/lib/python3.11/socketserver.py\", line 755, in __init__\n    self.handle()\n  File \"/usr/lib/python3.11/http/server.py\", line 436, in handle\n    self.handle_one_request()\n  File \"/usr/lib/python3.11/http/server.py\", line 424, in handle_one_request\n    method()\n  File \"/usr/lib/python3.11/http/server.py\", line 678, in do_GET\n    self.copyfile(f, self.wfile)\n  File \"/usr/lib/python3.11/http/server.py\", line 877, in copyfile\n    shutil.copyfileobj(source, outputfile)\n  File \"/usr/lib/python3.11/shutil.py\", line 200, in copyfileobj\n    fdst_write(buf)\n  File \"/usr/lib/python3.11/socketserver.py\", line 834, in write\n    self._sock.sendall(b)\nBrokenPipeError: [Errno 32] Broken pipe\n----------------------------------------\n\n\n\nimport dioxus_widget\nprint(\"Launching the widget server\")\ndioxus_widget.init(\n    \"pkg/features2image_diffusion_dioxus_widgets.js\",\n    style_path=\"pkg/style.css\",\n    verbose=False\n)\n\nLaunching the widget server\n\n\n(&lt;Thread(http server, started 128802332083904)&gt;,\n '&lt;!doctype html&gt;\\n&lt;html lang=\"en\"&gt;\\n  &lt;head&gt;\\n    &lt;script type=\"module\"&gt;\\n      import init, { COMPONENT } from \"/pkg/features2image_diffusion_dioxus_widgets.js\";\\n      async function main() {\\n        await init();\\n        COMPONENT(document.getElementById(\"dioxus-component\"));\\n      }\\n      main();\\n    &lt;/script&gt;\\n&lt;link rel=\"stylesheet\" href=\"/pkg/style.css\"&gt;\\n  &lt;/head&gt;\\n  &lt;body&gt;\\n    &lt;div id=\"dioxus-component\"ATTRIBUTES&gt;DATA&lt;/div&gt;\\n  &lt;/body&gt;\\n&lt;/html&gt;\\n')\n\n\n\nRUN_ID = \"4409b6282a7d05f0b08880228d6d6564011fa40be412073ff05aff8bf2dc49fa\"\ndioxus_widget.debug(\"feature_images\", \"\", {\"run\": RUN_ID}, width=\"60%\", height=\"2000px\")\n\n\n\n\n\n        \n        \n\n\n\ndioxus_widget.clean()"
  },
  {
    "objectID": "doc/exploratory data analysis.html",
    "href": "doc/exploratory data analysis.html",
    "title": "",
    "section": "",
    "text": ":wasm feature-images\n:dep wasm-bindgen = \"*\"\n:dep features2image_diffusion-widget = { path = \"../widget\" }\n:dep web-sys = { version = \"*\", features = [\"HtmlElement\"] }\nuse wasm_bindgen::prelude::*;\nuse features2image_diffusion_widget;\nuse web_sys::HtmlElement;\n\n\n\n\n\n\n\n:dep rand = \"*\"\nuse rand::distributions::{Alphanumeric, DistString};\n\nfn display_js(js: &str) {\n    let id = Alphanumeric.sample_string(&mut rand::thread_rng(), 16);\n    println!(\"EVCXR_BEGIN_CONTENT text/html\\n\n    &lt;div id='{id}'&gt;&lt;/div&gt;\n    &lt;script&gt;\n        async function __evcxr_display() {{\n            evcxr = window.evcxr;\n            cwd = window.evcxr_cwd;\n            root = document.getElementById('{id}');\n            {js}\n        }};\n        window.on_load = __evcxr_display;\n        __evcxr_display();\n    &lt;/script&gt;\n    \\nEVCXR_END_CONTENT\");\n}\n\n\n:dep features2image_diffusion-read-result = { path = \"../read-result\" }\nuse features2image_diffusion_read_result::create_file_tensor;\nuse features2image_diffusion_read_result::f2id_fs::Files;\nuse features2image_diffusion_read_result::list_paths::list_paths;\n\nlet RUN_ID = \"4409b6282a7d05f0b08880228d6d6564011fa40be412073ff05aff8bf2dc49fa\";\nlet RUN_DIR = format!(\"../xyz/run/{RUN_ID}\");\n\n\nlet files: Files = create_file_tensor(&RUN_DIR).unwrap();\nstd::fs::write(\"./files.bytes\", files.to_bytes()?)?;\nfiles.dfmt.shape()\n\n[18, 256, 4, 3]\n\n\n\nlet RUN_ID = \"4409b6282a7d05f0b08880228d6d6564011fa40be412073ff05aff8bf2dc49fa\";\nlet RUN_DIR = format!(\"../xyz/run/{RUN_ID}\");\n\ndisplay_js(&format!(\"\nresp = await fetch(cwd + 'files.bytes');\nevcxr.feature_images(\n    root,\n    new Uint8Array(await resp.arrayBuffer()),\n    cwd + '{RUN_DIR}'\n);\n\"));"
  }
]