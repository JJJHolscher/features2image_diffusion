[
  {
    "objectID": "exploratory data analysis.html",
    "href": "exploratory data analysis.html",
    "title": "",
    "section": "",
    "text": ":wasm feature-images\n:dep wasm-bindgen = \"*\"\n:dep features2image_diffusion-widget = { path = \"../widget\" }\n:dep web-sys = { version = \"*\", features = [\"HtmlElement\"] }\nuse wasm_bindgen::prelude::*;\nuse features2image_diffusion_widget;\nuse web_sys::HtmlElement;\n\n\n\n\n\n\n\n\n:dep features2image_diffusion-read-result = { path = \"../read-result\" }\n:dep evcxr_ssg = \"*\"\nuse evcxr_ssg::call_wasm;\nuse features2image_diffusion_read_result::{\n    create_file_tensor,\n    f2id_fs::Files,\n    list_paths::list_paths\n};\nevcxr_ssg::stylesheet(\"../web/tailwind.css\");\nlet RUN_ID = \"4409b6282a7d05f0b08880228d6d6564011fa40be412073ff05aff8bf2dc49fa\";\nlet RUN_DIR = format!(\"../xyz/run/{RUN_ID}\");\n\n\n\n\n\nlet files: Files = create_file_tensor(&RUN_DIR).unwrap();\nfiles.dfmt.shape()\n\n[18, 256, 4, 3]\n\n\n\ncall_wasm!(\"feature_images\", \"$\", &RUN_DIR, &files);"
  },
  {
    "objectID": "ddpm.html",
    "href": "ddpm.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Today, we’re reading and summarizing Ho et al. I believe their paper to be the first time diffusion models were used for image generation.\n\n\n\nHere is their figure on how images are generated with diffusion\n\n\nThe important terms:\n\\(\\textbf{x}_0\\) is the data, \\(\\textbf{x}_T\\) is pure noise and any \\(\\textbf{x}_{0&lt;t&lt;T}\\) is the data mixed with the noise.\n\\(q\\) is the forward process, the process of gradually adding Gaussian noise to the data.\n\\(p\\) is the parameterized backward process, the process of removing noise from data.\n\n\n Where \\(\\beta_t\\), the amount of noise added at time step \\(t\\), can be learned or held constant. For simplicity, the authors keep it constant in this paper.\n\\(q\\) can then be generalized to  where \nSince \\(\\beta_t\\) is held constant, the forward process term \\(L_T\\) in the upcoming loss function is kept constant.\n\n\n\nThe complete loss function decomposes into 3 parts.  \\(L_T\\) is the ignored loss component of the forward process,\n\\(L_{t-1}\\) is the reverse process and\n\\(L_0\\) is the reverse process decoder.\n\\(L_{t-1}\\) is rewritten to  which, when expressed into pseudocode, is  Intuitively, the model \\(\\epsilon_\\theta\\) minimizes loss by outputting the noise \\(\\epsilon\\) that was added to it’s input data, weighted by the forward process’s variance at that time step.\n\\(L_0\\) is the last reverse process step. It is responsible for scaling \\(x_1\\) to RGB values.\n\n\n\nTogether, the reverse process is simplified to  where \\(t\\) is uniform between 1 and \\(T\\). The simplification emphasizes loss at larger \\(t\\)’s (when there is more noise) since those are harder for the model to predict."
  },
  {
    "objectID": "generate-imagenet.html",
    "href": "generate-imagenet.html",
    "title": "Generating ImageNet Images",
    "section": "",
    "text": "Here we go through the steps to get a trained model to generate ImageNet images.\n\n%cd ..\n\n/home/jo3/p/features2image_diffusion\n\n\n/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning:\n\nusing dhist requires you to install the `pickleshare` library.\n\n\n\nOur dataset, ImageNetSet will return features, images and labels, therefore we need to tell it the location of the features and the location of the stored images and labels.\nWe give an additional argument that gets passed to the torchvision.transform.Resize function, to reduce the size of the outputted images.\n\nfrom features2image_diffusion.data import ImageNetSet\n\ndataset = ImageNetSet(\n    \"../sparse_autoencoder/run/1ae3fe65/train\",\n    \"../sparse_autoencoder/res/imagenet/train\",\n    image_size=64,\n)\n\n\n\n\n\nfeatures, image, label = dataset[0]\nprint(features.shape, image.shape, type(label))\n\nnum_features = features.shape[0]\nn_channels, _, img_len = image.shape\nnum_features, n_channels, img_len\n\ntorch.Size([40768]) torch.Size([3, 64, 64]) &lt;class 'int'&gt;\n\n\n(40768, 3, 64)\n\n\n\nfrom features2image_diffusion.unet import load_ddpm\n\nddpm = load_ddpm(\n    \"run/f2a2e4ac/model/epoch-39.pth\",\n    n_classes=num_features,\n    n_channels=n_channels,\n    img_len=img_len,\n    hidden_size=128,\n    diffusion_steps=400,\n    device=\"cpu\",\n).eval()\n\n\nimport torch\nfrom torch import Tensor\n\nwith torch.no_grad():\n    generations, _ = ddpm.sample(\n        context=Tensor(features),\n        n_sample=3,\n        size=(3, 64, 64),\n        device=\"cpu\",\n        verbose=True,\n        store=False,\n    )\n\ngenerations.shape\n\n\nsampling timestep 400\n\n\nRuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 256490078208 bytes. Error code 12 (Cannot allocate memory)\n\n\n\nfrom jo3mnist.vis import to_img\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick"
  },
  {
    "objectID": "show-images.html",
    "href": "show-images.html",
    "title": "",
    "section": "",
    "text": "%cd ..\n\n/home/jo3/p/features2image_diffusion\n\n\n/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning:\n\nusing dhist requires you to install the `pickleshare` library.\n\n\n\n\nimport numpy as np\ngenerations = np.load(\"run/some-weird-gens/1/0/0-generations.npy\")\n\n\n\nimport matplotlib.pyplot as plt\nfrom torch import Tensor\n\ndef prepare_image(img, min_=-2.1008, max_=2.6400):\n    # channels, height, width  -&gt;  height, width, channels\n    img = img.permute(1, 2, 0)\n    # have the image's values range between 0 and 1\n    img = (img - min_) / (max_ - min_)\n    return img\n    \nfig, axes = plt.subplots(2,2)\nfor i, ax in enumerate(axes.flat):\n    image = prepare_image(Tensor(generations[i]))\n    ax.axis('off')\n    ax.imshow(image)\n\nplt.suptitle(\"generated images\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ddpm.html#the-forward-process",
    "href": "ddpm.html#the-forward-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Where \\(\\beta_t\\), the amount of noise added at time step \\(t\\), can be learned or held constant. For simplicity, the authors keep it constant in this paper.\n\\(q\\) can then be generalized to  where \nSince \\(\\beta_t\\) is held constant, the forward process term \\(L_T\\) in the upcoming loss function is kept constant."
  },
  {
    "objectID": "ddpm.html#the-reverse-process",
    "href": "ddpm.html#the-reverse-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The complete loss function decomposes into 3 parts.  \\(L_T\\) is the ignored loss component of the forward process,\n\\(L_{t-1}\\) is the reverse process and\n\\(L_0\\) is the reverse process decoder.\n\\(L_{t-1}\\) is rewritten to  which, when expressed into pseudocode, is  Intuitively, the model \\(\\epsilon_\\theta\\) minimizes loss by outputting the noise \\(\\epsilon\\) that was added to it’s input data, weighted by the forward process’s variance at that time step.\n\\(L_0\\) is the last reverse process step. It is responsible for scaling \\(x_1\\) to RGB values."
  },
  {
    "objectID": "ddpm.html#simplified",
    "href": "ddpm.html#simplified",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Together, the reverse process is simplified to  where \\(t\\) is uniform between 1 and \\(T\\). The simplification emphasizes loss at larger \\(t\\)’s (when there is more noise) since those are harder for the model to predict."
  },
  {
    "objectID": "torch_ddpm_to_equinox.html",
    "href": "torch_ddpm_to_equinox.html",
    "title": "Loading torch weights into equinox",
    "section": "",
    "text": "To make the openai diffusion model work in equinox, I’ve had to rewrite the code but I also a way to load the torch weights into my equinox python class.\nThat’s what this code will be for.\nHere are the arguments of the model of interest:\n\nimport jax\njax.config.update(\"jax_platform_name\", \"cpu\")\njax.config.update(\"jax_enable_x64\", True)\n\nWEIGHTS_PATH = \"res/imagenet64_cond_270M_250K.pt\"\nARGUMENTS = {\n    \"lr\": 3e-4,\n    \"batch_size\": 128,\n    \"architecture\": {\n        \"image_size\": 64,\n        \"num_channels\": 192,\n        \"num_res_blocks\": 3,\n        \"learn_sigma\": True,\n        \"class_cond\": True,\n        \"diffusion_steps\": 4000,\n        \"noise_schedule\": \"cosine\",\n        \"rescale_learned_sigmas\": False,\n        \"rescale_timesteps\": False\n    }\n}\n\nLet’s get it’s weights\n\nimport torch as th\n\nsaved_weights = th.load(WEIGHTS_PATH, map_location=th.device(\"cpu\"))\n\nand filter out any stateful layers.\n\nimport jax.dlpack as jdp\n\ndef to_jax(tensor):\n    return jdp.from_dlpack(th.to_dlpack(tensor))\n\n# iterate over stateless weights\nweights = [\n    (name, to_jax(weight))\n    for name, weight in saved_weights.items()\n    if \"num_batches\" not in name and \"running\" not in name\n]\n\n# iterate over stateful layers\nrunning_mean = None\nbn_s = []\nfor name, weight in saved_weights.items():\n    if \"running_mean\" in name:\n        bn_s.append(False)\n        assert running_mean is None\n        running_mean = to_jax(weight)\n    elif \"running_var\" in name:\n        assert running_mean is not None\n        bn_s.append((running_mean, to_jax(weight)))\n        running_mean = None\n\ndel saved_weights\n\n# luckily there are no stateful layers!\nprint(bn_s)\n\nCUDA backend failed to initialize: jaxlib/cuda/versions_helpers.cc:98: operation cuInit(0) failed: CUDA_ERROR_NO_DEVICE (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n[]\n\n\nNow we create an equinox U-net with the correct arguments.\n\nimport argtoml\nimport jax.random as jrd\nfrom improved_diffusion_equinox import script_util\n\n\nargs = argtoml.parse_args(\"improved-diffusion.toml\", overwrite=ARGUMENTS)\n\nkey = jrd.PRNGKey(args[\"seed\"])\nkey, subkey = jrd.split(key, 2)\nargs[\"architecture\"][\"key\"] = subkey\n\nmodel, diffusion = script_util.create_model_and_diffusion(**args[\"architecture\"])\nmodel\n\nUNetModel(\n  in_channels=3,\n  model_channels=192,\n  out_channels=6,\n  num_res_blocks=3,\n  attention_resolutions=(4, 8),\n  dropout=0.0,\n  channel_mult=(1, 2, 3, 4),\n  conv_resample=True,\n  num_classes=1000,\n  num_heads=4,\n  num_heads_upsample=4,\n  time_embed=Sequential(\n    layers=(\n      Linear(\n        weight=f64[768,192],\n        bias=f64[768],\n        in_features=192,\n        out_features=768,\n        use_bias=True\n      ),\n      &lt;wrapped function silu&gt;,\n      Linear(\n        weight=f64[768,768],\n        bias=f64[768],\n        in_features=768,\n        out_features=768,\n        use_bias=True\n      )\n    )\n  ),\n  label_emb=Embedding(\n    num_embeddings=1000,\n    embedding_size=768,\n    weight=f64[1000,768]\n  ),\n  input_blocks=[\n    TimestepEmbedSequential(\n      layers=[\n        Conv(\n          num_spatial_dims=2,\n          weight=f64[192,3,3,3],\n          bias=f64[192,1,1],\n          in_channels=3,\n          out_channels=192,\n          kernel_size=(3, 3),\n          stride=(1, 1),\n          padding=((1, 1), (1, 1)),\n          dilation=(1, 1),\n          groups=1,\n          use_bias=True,\n          padding_mode='ZEROS'\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=192,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=192,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[384,768],\n                bias=f64[384],\n                in_features=768,\n                out_features=384,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=192,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=192,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[384,768],\n                bias=f64[384],\n                in_features=768,\n                out_features=384,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=192,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=192,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[384,768],\n                bias=f64[384],\n                in_features=768,\n                out_features=384,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        Downsample(\n          channels=192,\n          use_conv=True,\n          dims=2,\n          op=Conv(\n            num_spatial_dims=2,\n            weight=f64[192,192,3,3],\n            bias=f64[192,1,1],\n            in_channels=192,\n            out_channels=192,\n            kernel_size=(3, 3),\n            stride=(2, 2),\n            padding=((1, 1), (1, 1)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=192,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=384,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,192,3,3],\n                bias=f64[384,1,1],\n                in_channels=192,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[768,768],\n                bias=f64[768],\n                in_features=768,\n                out_features=768,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[384,192,1,1],\n            bias=f64[384,1,1],\n            in_channels=192,\n            out_channels=384,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=384,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=384,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[768,768],\n                bias=f64[768],\n                in_features=768,\n                out_features=768,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=384,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=384,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[768,768],\n                bias=f64[768],\n                in_features=768,\n                out_features=768,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        Downsample(\n          channels=384,\n          use_conv=True,\n          dims=2,\n          op=Conv(\n            num_spatial_dims=2,\n            weight=f64[384,384,3,3],\n            bias=f64[384,1,1],\n            in_channels=384,\n            out_channels=384,\n            kernel_size=(3, 3),\n            stride=(2, 2),\n            padding=((1, 1), (1, 1)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=384,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=576,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,384,3,3],\n                bias=f64[576,1,1],\n                in_channels=384,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1152,768],\n                bias=f64[1152],\n                in_features=768,\n                out_features=1152,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[576,384,1,1],\n            bias=f64[576,1,1],\n            in_channels=384,\n            out_channels=576,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=576,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=576,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[576],\n            bias=f64[576]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[1728,576,1],\n            bias=f64[1728,1],\n            in_channels=576,\n            out_channels=1728,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[576,576,1],\n            bias=f64[576,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=576,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=576,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1152,768],\n                bias=f64[1152],\n                in_features=768,\n                out_features=1152,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        ),\n        AttentionBlock(\n          channels=576,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=576,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[576],\n            bias=f64[576]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[1728,576,1],\n            bias=f64[1728,1],\n            in_channels=576,\n            out_channels=1728,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[576,576,1],\n            bias=f64[576,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=576,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=576,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1152,768],\n                bias=f64[1152],\n                in_features=768,\n                out_features=1152,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        ),\n        AttentionBlock(\n          channels=576,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=576,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[576],\n            bias=f64[576]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[1728,576,1],\n            bias=f64[1728,1],\n            in_channels=576,\n            out_channels=1728,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[576,576,1],\n            bias=f64[576,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        Downsample(\n          channels=576,\n          use_conv=True,\n          dims=2,\n          op=Conv(\n            num_spatial_dims=2,\n            weight=f64[576,576,3,3],\n            bias=f64[576,1,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(3, 3),\n            stride=(2, 2),\n            padding=((1, 1), (1, 1)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=576,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=768,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,576,3,3],\n                bias=f64[768,1,1],\n                in_channels=576,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1536,768],\n                bias=f64[1536],\n                in_features=768,\n                out_features=1536,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[768,576,1,1],\n            bias=f64[768,1,1],\n            in_channels=576,\n            out_channels=768,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=768,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=768,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[768],\n            bias=f64[768]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[2304,768,1],\n            bias=f64[2304,1],\n            in_channels=768,\n            out_channels=2304,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[768,768,1],\n            bias=f64[768,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=768,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=768,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1536,768],\n                bias=f64[1536],\n                in_features=768,\n                out_features=1536,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        ),\n        AttentionBlock(\n          channels=768,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=768,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[768],\n            bias=f64[768]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[2304,768,1],\n            bias=f64[2304,1],\n            in_channels=768,\n            out_channels=2304,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[768,768,1],\n            bias=f64[768,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=768,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=768,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1536,768],\n                bias=f64[1536],\n                in_features=768,\n                out_features=1536,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Identity()\n        ),\n        AttentionBlock(\n          channels=768,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=768,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[768],\n            bias=f64[768]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[2304,768,1],\n            bias=f64[2304,1],\n            in_channels=768,\n            out_channels=2304,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[768,768,1],\n            bias=f64[768,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    )\n  ],\n  middle_block=TimestepEmbedSequential(\n    layers=[\n      ResBlock(\n        channels=768,\n        emb_channels=768,\n        dropout=0.0,\n        out_channels=768,\n        use_conv=False,\n        use_scale_shift_norm=True,\n        in_layers=Sequential(\n          layers=(\n            GroupNorm(\n              groups=32,\n              channels=768,\n              eps=1e-05,\n              channelwise_affine=True,\n              weight=f64[768],\n              bias=f64[768]\n            ),\n            &lt;wrapped function silu&gt;,\n            Conv(\n              num_spatial_dims=2,\n              weight=f64[768,768,3,3],\n              bias=f64[768,1,1],\n              in_channels=768,\n              out_channels=768,\n              kernel_size=(3, 3),\n              stride=(1, 1),\n              padding=((1, 1), (1, 1)),\n              dilation=(1, 1),\n              groups=1,\n              use_bias=True,\n              padding_mode='ZEROS'\n            )\n          )\n        ),\n        emb_layers=Sequential(\n          layers=(\n            &lt;wrapped function silu&gt;,\n            Linear(\n              weight=f64[1536,768],\n              bias=f64[1536],\n              in_features=768,\n              out_features=1536,\n              use_bias=True\n            )\n          )\n        ),\n        out_layers=Sequential(\n          layers=(\n            GroupNorm(\n              groups=32,\n              channels=768,\n              eps=1e-05,\n              channelwise_affine=True,\n              weight=f64[768],\n              bias=f64[768]\n            ),\n            &lt;wrapped function silu&gt;,\n            Dropout(p=0.0, inference=False),\n            Conv(\n              num_spatial_dims=2,\n              weight=f64[768,768,3,3],\n              bias=f64[768,1,1],\n              in_channels=768,\n              out_channels=768,\n              kernel_size=(3, 3),\n              stride=(1, 1),\n              padding=((1, 1), (1, 1)),\n              dilation=(1, 1),\n              groups=1,\n              use_bias=True,\n              padding_mode='ZEROS'\n            )\n          )\n        ),\n        skip_connection=Identity()\n      ),\n      AttentionBlock(\n        channels=768,\n        num_heads=4,\n        norm=GroupNorm(\n          groups=32,\n          channels=768,\n          eps=1e-05,\n          channelwise_affine=True,\n          weight=f64[768],\n          bias=f64[768]\n        ),\n        qkv=Conv(\n          num_spatial_dims=1,\n          weight=f64[2304,768,1],\n          bias=f64[2304,1],\n          in_channels=768,\n          out_channels=2304,\n          kernel_size=(1,),\n          stride=(1,),\n          padding=((0, 0),),\n          dilation=(1,),\n          groups=1,\n          use_bias=True,\n          padding_mode='ZEROS'\n        ),\n        attention=QKVAttention(),\n        proj_out=Conv(\n          num_spatial_dims=1,\n          weight=f64[768,768,1],\n          bias=f64[768,1],\n          in_channels=768,\n          out_channels=768,\n          kernel_size=(1,),\n          stride=(1,),\n          padding=((0, 0),),\n          dilation=(1,),\n          groups=1,\n          use_bias=True,\n          padding_mode='ZEROS'\n        )\n      ),\n      ResBlock(\n        channels=768,\n        emb_channels=768,\n        dropout=0.0,\n        out_channels=768,\n        use_conv=False,\n        use_scale_shift_norm=True,\n        in_layers=Sequential(\n          layers=(\n            GroupNorm(\n              groups=32,\n              channels=768,\n              eps=1e-05,\n              channelwise_affine=True,\n              weight=f64[768],\n              bias=f64[768]\n            ),\n            &lt;wrapped function silu&gt;,\n            Conv(\n              num_spatial_dims=2,\n              weight=f64[768,768,3,3],\n              bias=f64[768,1,1],\n              in_channels=768,\n              out_channels=768,\n              kernel_size=(3, 3),\n              stride=(1, 1),\n              padding=((1, 1), (1, 1)),\n              dilation=(1, 1),\n              groups=1,\n              use_bias=True,\n              padding_mode='ZEROS'\n            )\n          )\n        ),\n        emb_layers=Sequential(\n          layers=(\n            &lt;wrapped function silu&gt;,\n            Linear(\n              weight=f64[1536,768],\n              bias=f64[1536],\n              in_features=768,\n              out_features=1536,\n              use_bias=True\n            )\n          )\n        ),\n        out_layers=Sequential(\n          layers=(\n            GroupNorm(\n              groups=32,\n              channels=768,\n              eps=1e-05,\n              channelwise_affine=True,\n              weight=f64[768],\n              bias=f64[768]\n            ),\n            &lt;wrapped function silu&gt;,\n            Dropout(p=0.0, inference=False),\n            Conv(\n              num_spatial_dims=2,\n              weight=f64[768,768,3,3],\n              bias=f64[768,1,1],\n              in_channels=768,\n              out_channels=768,\n              kernel_size=(3, 3),\n              stride=(1, 1),\n              padding=((1, 1), (1, 1)),\n              dilation=(1, 1),\n              groups=1,\n              use_bias=True,\n              padding_mode='ZEROS'\n            )\n          )\n        ),\n        skip_connection=Identity()\n      )\n    ]\n  ),\n  output_blocks=[\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=1536,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=768,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=1536,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[1536],\n                bias=f64[1536]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,1536,3,3],\n                bias=f64[768,1,1],\n                in_channels=1536,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1536,768],\n                bias=f64[1536],\n                in_features=768,\n                out_features=1536,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[768,1536,1,1],\n            bias=f64[768,1,1],\n            in_channels=1536,\n            out_channels=768,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=768,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=768,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[768],\n            bias=f64[768]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[2304,768,1],\n            bias=f64[2304,1],\n            in_channels=768,\n            out_channels=2304,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[768,768,1],\n            bias=f64[768,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=1536,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=768,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=1536,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[1536],\n                bias=f64[1536]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,1536,3,3],\n                bias=f64[768,1,1],\n                in_channels=1536,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1536,768],\n                bias=f64[1536],\n                in_features=768,\n                out_features=1536,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[768,1536,1,1],\n            bias=f64[768,1,1],\n            in_channels=1536,\n            out_channels=768,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=768,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=768,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[768],\n            bias=f64[768]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[2304,768,1],\n            bias=f64[2304,1],\n            in_channels=768,\n            out_channels=2304,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[768,768,1],\n            bias=f64[768,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=1536,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=768,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=1536,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[1536],\n                bias=f64[1536]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,1536,3,3],\n                bias=f64[768,1,1],\n                in_channels=1536,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1536,768],\n                bias=f64[1536],\n                in_features=768,\n                out_features=1536,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[768,1536,1,1],\n            bias=f64[768,1,1],\n            in_channels=1536,\n            out_channels=768,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=768,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=768,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[768],\n            bias=f64[768]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[2304,768,1],\n            bias=f64[2304,1],\n            in_channels=768,\n            out_channels=2304,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[768,768,1],\n            bias=f64[768,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=1344,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=768,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=1344,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[1344],\n                bias=f64[1344]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,1344,3,3],\n                bias=f64[768,1,1],\n                in_channels=1344,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1536,768],\n                bias=f64[1536],\n                in_features=768,\n                out_features=1536,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[768,768,3,3],\n                bias=f64[768,1,1],\n                in_channels=768,\n                out_channels=768,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[768,1344,1,1],\n            bias=f64[768,1,1],\n            in_channels=1344,\n            out_channels=768,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=768,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=768,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[768],\n            bias=f64[768]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[2304,768,1],\n            bias=f64[2304,1],\n            in_channels=768,\n            out_channels=2304,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[768,768,1],\n            bias=f64[768,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        Upsample(\n          channels=768,\n          dims=2,\n          conv=Conv(\n            num_spatial_dims=2,\n            weight=f64[768,768,3,3],\n            bias=f64[768,1,1],\n            in_channels=768,\n            out_channels=768,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=1344,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=576,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=1344,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[1344],\n                bias=f64[1344]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,1344,3,3],\n                bias=f64[576,1,1],\n                in_channels=1344,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1152,768],\n                bias=f64[1152],\n                in_features=768,\n                out_features=1152,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[576,1344,1,1],\n            bias=f64[576,1,1],\n            in_channels=1344,\n            out_channels=576,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=576,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=576,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[576],\n            bias=f64[576]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[1728,576,1],\n            bias=f64[1728,1],\n            in_channels=576,\n            out_channels=1728,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[576,576,1],\n            bias=f64[576,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=1152,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=576,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=1152,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[1152],\n                bias=f64[1152]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,1152,3,3],\n                bias=f64[576,1,1],\n                in_channels=1152,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1152,768],\n                bias=f64[1152],\n                in_features=768,\n                out_features=1152,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[576,1152,1,1],\n            bias=f64[576,1,1],\n            in_channels=1152,\n            out_channels=576,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=576,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=576,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[576],\n            bias=f64[576]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[1728,576,1],\n            bias=f64[1728,1],\n            in_channels=576,\n            out_channels=1728,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[576,576,1],\n            bias=f64[576,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=1152,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=576,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=1152,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[1152],\n                bias=f64[1152]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,1152,3,3],\n                bias=f64[576,1,1],\n                in_channels=1152,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1152,768],\n                bias=f64[1152],\n                in_features=768,\n                out_features=1152,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[576,1152,1,1],\n            bias=f64[576,1,1],\n            in_channels=1152,\n            out_channels=576,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=576,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=576,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[576],\n            bias=f64[576]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[1728,576,1],\n            bias=f64[1728,1],\n            in_channels=576,\n            out_channels=1728,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[576,576,1],\n            bias=f64[576,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=960,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=576,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=960,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[960],\n                bias=f64[960]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,960,3,3],\n                bias=f64[576,1,1],\n                in_channels=960,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[1152,768],\n                bias=f64[1152],\n                in_features=768,\n                out_features=1152,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[576,576,3,3],\n                bias=f64[576,1,1],\n                in_channels=576,\n                out_channels=576,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[576,960,1,1],\n            bias=f64[576,1,1],\n            in_channels=960,\n            out_channels=576,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        AttentionBlock(\n          channels=576,\n          num_heads=4,\n          norm=GroupNorm(\n            groups=32,\n            channels=576,\n            eps=1e-05,\n            channelwise_affine=True,\n            weight=f64[576],\n            bias=f64[576]\n          ),\n          qkv=Conv(\n            num_spatial_dims=1,\n            weight=f64[1728,576,1],\n            bias=f64[1728,1],\n            in_channels=576,\n            out_channels=1728,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          ),\n          attention=QKVAttention(),\n          proj_out=Conv(\n            num_spatial_dims=1,\n            weight=f64[576,576,1],\n            bias=f64[576,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=((0, 0),),\n            dilation=(1,),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        Upsample(\n          channels=576,\n          dims=2,\n          conv=Conv(\n            num_spatial_dims=2,\n            weight=f64[576,576,3,3],\n            bias=f64[576,1,1],\n            in_channels=576,\n            out_channels=576,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=960,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=384,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=960,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[960],\n                bias=f64[960]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,960,3,3],\n                bias=f64[384,1,1],\n                in_channels=960,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[768,768],\n                bias=f64[768],\n                in_features=768,\n                out_features=768,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[384,960,1,1],\n            bias=f64[384,1,1],\n            in_channels=960,\n            out_channels=384,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=768,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=384,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,768,3,3],\n                bias=f64[384,1,1],\n                in_channels=768,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[768,768],\n                bias=f64[768],\n                in_features=768,\n                out_features=768,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[384,768,1,1],\n            bias=f64[384,1,1],\n            in_channels=768,\n            out_channels=384,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=768,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=384,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=768,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[768],\n                bias=f64[768]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,768,3,3],\n                bias=f64[384,1,1],\n                in_channels=768,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[768,768],\n                bias=f64[768],\n                in_features=768,\n                out_features=768,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[384,768,1,1],\n            bias=f64[384,1,1],\n            in_channels=768,\n            out_channels=384,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=576,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=384,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,576,3,3],\n                bias=f64[384,1,1],\n                in_channels=576,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[768,768],\n                bias=f64[768],\n                in_features=768,\n                out_features=768,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[384,384,3,3],\n                bias=f64[384,1,1],\n                in_channels=384,\n                out_channels=384,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[384,576,1,1],\n            bias=f64[384,1,1],\n            in_channels=576,\n            out_channels=384,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        ),\n        Upsample(\n          channels=384,\n          dims=2,\n          conv=Conv(\n            num_spatial_dims=2,\n            weight=f64[384,384,3,3],\n            bias=f64[384,1,1],\n            in_channels=384,\n            out_channels=384,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=576,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=192,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=576,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[576],\n                bias=f64[576]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,576,3,3],\n                bias=f64[192,1,1],\n                in_channels=576,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[384,768],\n                bias=f64[384],\n                in_features=768,\n                out_features=384,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[192,576,1,1],\n            bias=f64[192,1,1],\n            in_channels=576,\n            out_channels=192,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=384,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=192,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,384,3,3],\n                bias=f64[192,1,1],\n                in_channels=384,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[384,768],\n                bias=f64[384],\n                in_features=768,\n                out_features=384,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[192,384,1,1],\n            bias=f64[192,1,1],\n            in_channels=384,\n            out_channels=192,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=384,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=192,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,384,3,3],\n                bias=f64[192,1,1],\n                in_channels=384,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[384,768],\n                bias=f64[384],\n                in_features=768,\n                out_features=384,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[192,384,1,1],\n            bias=f64[192,1,1],\n            in_channels=384,\n            out_channels=192,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    ),\n    TimestepEmbedSequential(\n      layers=[\n        ResBlock(\n          channels=384,\n          emb_channels=768,\n          dropout=0.0,\n          out_channels=192,\n          use_conv=False,\n          use_scale_shift_norm=True,\n          in_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=384,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[384],\n                bias=f64[384]\n              ),\n              &lt;wrapped function silu&gt;,\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,384,3,3],\n                bias=f64[192,1,1],\n                in_channels=384,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          emb_layers=Sequential(\n            layers=(\n              &lt;wrapped function silu&gt;,\n              Linear(\n                weight=f64[384,768],\n                bias=f64[384],\n                in_features=768,\n                out_features=384,\n                use_bias=True\n              )\n            )\n          ),\n          out_layers=Sequential(\n            layers=(\n              GroupNorm(\n                groups=32,\n                channels=192,\n                eps=1e-05,\n                channelwise_affine=True,\n                weight=f64[192],\n                bias=f64[192]\n              ),\n              &lt;wrapped function silu&gt;,\n              Dropout(p=0.0, inference=False),\n              Conv(\n                num_spatial_dims=2,\n                weight=f64[192,192,3,3],\n                bias=f64[192,1,1],\n                in_channels=192,\n                out_channels=192,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=((1, 1), (1, 1)),\n                dilation=(1, 1),\n                groups=1,\n                use_bias=True,\n                padding_mode='ZEROS'\n              )\n            )\n          ),\n          skip_connection=Conv(\n            num_spatial_dims=2,\n            weight=f64[192,384,1,1],\n            bias=f64[192,1,1],\n            in_channels=384,\n            out_channels=192,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=((0, 0), (0, 0)),\n            dilation=(1, 1),\n            groups=1,\n            use_bias=True,\n            padding_mode='ZEROS'\n          )\n        )\n      ]\n    )\n  ],\n  out=Sequential(\n    layers=(\n      GroupNorm(\n        groups=32,\n        channels=192,\n        eps=1e-05,\n        channelwise_affine=True,\n        weight=f64[192],\n        bias=f64[192]\n      ),\n      &lt;wrapped function silu&gt;,\n      Conv(\n        num_spatial_dims=2,\n        weight=f64[6,192,3,3],\n        bias=f64[6,1,1],\n        in_channels=192,\n        out_channels=6,\n        kernel_size=(3, 3),\n        stride=(1, 1),\n        padding=((1, 1), (1, 1)),\n        dilation=(1, 1),\n        groups=1,\n        use_bias=True,\n        padding_mode='ZEROS'\n      )\n    )\n  )\n)\n\n\nAll that is left is to supplant every weight from the unet model with the loaded torch weights.\n\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\n\nleaves, tree_def = jtu.tree_flatten(model)\n\nweights_iterator = iter(weights)\nnew_leaves = []\nnum_weight_leaves = 0\nfor leaf in leaves:\n    if isinstance(leaf, jnp.ndarray) and not (\n        leaf.size == 1 and isinstance(leaf.item(), bool)\n    ):\n        (weight_name, new_weights) = next(weights_iterator)\n        new_leaves.append(jnp.reshape(new_weights, leaf.shape))\n        num_weight_leaves += 1\n    else:\n        new_leaves.append(leaf)\n\nassert num_weight_leaves == len(weights)\n\nmodel = jtu.tree_unflatten(tree_def, new_leaves)\n\nfinally we save\n\nimport jo3util.eqx as jo3eqx\njo3eqx.save(WEIGHTS_PATH[:-2] + \"eqx\", model, args[\"architecture\"])"
  }
]