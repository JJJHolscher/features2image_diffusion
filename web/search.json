[
  {
    "objectID": "exploratory data analysis.html",
    "href": "exploratory data analysis.html",
    "title": "",
    "section": "",
    "text": ":wasm feature-images\n:dep wasm-bindgen = \"*\"\n:dep features2image_diffusion-widget = { path = \"../widget\" }\n:dep web-sys = { version = \"*\", features = [\"HtmlElement\"] }\nuse wasm_bindgen::prelude::*;\nuse features2image_diffusion_widget;\nuse web_sys::HtmlElement;\n\n\n\n\n\n\n\n\n:dep features2image_diffusion-read-result = { path = \"../read-result\" }\n:dep evcxr_ssg = \"*\"\nuse evcxr_ssg::call_wasm;\nuse features2image_diffusion_read_result::{\n    create_file_tensor,\n    f2id_fs::Files,\n    list_paths::list_paths\n};\nevcxr_ssg::stylesheet(\"../web/tailwind.css\");\nlet RUN_ID = \"4409b6282a7d05f0b08880228d6d6564011fa40be412073ff05aff8bf2dc49fa\";\nlet RUN_DIR = format!(\"../xyz/run/{RUN_ID}\");\n\n\n\n\n\nlet files: Files = create_file_tensor(&RUN_DIR).unwrap();\nfiles.dfmt.shape()\n\n[18, 256, 4, 3]\n\n\n\ncall_wasm!(\"feature_images\", \"$\", &RUN_DIR, &files);"
  },
  {
    "objectID": "ddpm.html",
    "href": "ddpm.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Today, we’re reading and summarizing Ho et al. I believe their paper to be the first time diffusion models were used for image generation.\n\n\n\nHere is their figure on how images are generated with diffusion\n\n\nThe important terms:\n\\(\\textbf{x}_0\\) is the data, \\(\\textbf{x}_T\\) is pure noise and any \\(\\textbf{x}_{0&lt;t&lt;T}\\) is the data mixed with the noise.\n\\(q\\) is the forward process, the process of gradually adding Gaussian noise to the data.\n\\(p\\) is the parameterized backward process, the process of removing noise from data.\n\n\n Where \\(\\beta_t\\), the amount of noise added at time step \\(t\\), can be learned or held constant. For simplicity, the authors keep it constant in this paper.\n\\(q\\) can then be generalized to  where \nSince \\(\\beta_t\\) is held constant, the forward process term \\(L_T\\) in the upcoming loss function is kept constant.\n\n\n\nThe complete loss function decomposes into 3 parts.  \\(L_T\\) is the ignored loss component of the forward process,\n\\(L_{t-1}\\) is the reverse process and\n\\(L_0\\) is the reverse process decoder.\n\\(L_{t-1}\\) is rewritten to  which, when expressed into pseudocode, is  Intuitively, the model \\(\\epsilon_\\theta\\) minimizes loss by outputting the noise \\(\\epsilon\\) that was added to it’s input data, weighted by the forward process’s variance at that time step.\n\\(L_0\\) is the last reverse process step. It is responsible for scaling \\(x_1\\) to RGB values.\n\n\n\nTogether, the reverse process is simplified to  where \\(t\\) is uniform between 1 and \\(T\\). The simplification emphasizes loss at larger \\(t\\)’s (when there is more noise) since those are harder for the model to predict."
  },
  {
    "objectID": "generate-imagenet.html",
    "href": "generate-imagenet.html",
    "title": "Generating ImageNet Images",
    "section": "",
    "text": "Here we go through the steps to get a trained model to generate ImageNet images.\n\n%cd ..\n\n/home/jo3/p/features2image_diffusion\n\n\n/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning:\n\nusing dhist requires you to install the `pickleshare` library.\n\n\n\nOur dataset, ImageNetSet will return features, images and labels, therefore we need to tell it the location of the features and the location of the stored images and labels.\nWe give an additional argument that gets passed to the torchvision.transform.Resize function, to reduce the size of the outputted images.\n\nfrom features2image_diffusion.data import ImageNetSet\n\ndataset = ImageNetSet(\n    \"../sparse_autoencoder/run/1ae3fe65/train\",\n    \"../sparse_autoencoder/res/imagenet/train\",\n    image_size=64,\n)\n\n\n\n\n\nfeatures, image, label = dataset[0]\nprint(features.shape, image.shape, type(label))\n\nnum_features = features.shape[0]\nn_channels, _, img_len = image.shape\nnum_features, n_channels, img_len\n\ntorch.Size([40768]) torch.Size([3, 64, 64]) &lt;class 'int'&gt;\n\n\n(40768, 3, 64)\n\n\n\nfrom features2image_diffusion.unet import load_ddpm\n\nddpm = load_ddpm(\n    \"run/f2a2e4ac/model/epoch-39.pth\",\n    n_classes=num_features,\n    n_channels=n_channels,\n    img_len=img_len,\n    hidden_size=128,\n    diffusion_steps=400,\n    device=\"cpu\",\n).eval()\n\n\nimport torch\nfrom torch import Tensor\n\nwith torch.no_grad():\n    generations, _ = ddpm.sample(\n        context=Tensor(features),\n        n_sample=3,\n        size=(3, 64, 64),\n        device=\"cpu\",\n        verbose=True,\n        store=False,\n    )\n\ngenerations.shape\n\n\nsampling timestep 400\n\n\nRuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 256490078208 bytes. Error code 12 (Cannot allocate memory)\n\n\n\nfrom jo3mnist.vis import to_img\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick"
  },
  {
    "objectID": "show-images.html",
    "href": "show-images.html",
    "title": "",
    "section": "",
    "text": "%cd ..\n\n/home/jo3/p/features2image_diffusion\n\n\n/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning:\n\nusing dhist requires you to install the `pickleshare` library.\n\n\n\n\nimport numpy as np\ngenerations = np.load(\"run/some-weird-gens/1/0/0-generations.npy\")\n\n\n\nimport matplotlib.pyplot as plt\nfrom torch import Tensor\n\ndef prepare_image(img, min_=-2.1008, max_=2.6400):\n    # channels, height, width  -&gt;  height, width, channels\n    img = img.permute(1, 2, 0)\n    # have the image's values range between 0 and 1\n    img = (img - min_) / (max_ - min_)\n    return img\n    \nfig, axes = plt.subplots(2,2)\nfor i, ax in enumerate(axes.flat):\n    image = prepare_image(Tensor(generations[i]))\n    ax.axis('off')\n    ax.imshow(image)\n\nplt.suptitle(\"generated images\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ddpm.html#the-forward-process",
    "href": "ddpm.html#the-forward-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Where \\(\\beta_t\\), the amount of noise added at time step \\(t\\), can be learned or held constant. For simplicity, the authors keep it constant in this paper.\n\\(q\\) can then be generalized to  where \nSince \\(\\beta_t\\) is held constant, the forward process term \\(L_T\\) in the upcoming loss function is kept constant."
  },
  {
    "objectID": "ddpm.html#the-reverse-process",
    "href": "ddpm.html#the-reverse-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The complete loss function decomposes into 3 parts.  \\(L_T\\) is the ignored loss component of the forward process,\n\\(L_{t-1}\\) is the reverse process and\n\\(L_0\\) is the reverse process decoder.\n\\(L_{t-1}\\) is rewritten to  which, when expressed into pseudocode, is  Intuitively, the model \\(\\epsilon_\\theta\\) minimizes loss by outputting the noise \\(\\epsilon\\) that was added to it’s input data, weighted by the forward process’s variance at that time step.\n\\(L_0\\) is the last reverse process step. It is responsible for scaling \\(x_1\\) to RGB values."
  },
  {
    "objectID": "ddpm.html#simplified",
    "href": "ddpm.html#simplified",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Together, the reverse process is simplified to  where \\(t\\) is uniform between 1 and \\(T\\). The simplification emphasizes loss at larger \\(t\\)’s (when there is more noise) since those are harder for the model to predict."
  }
]