{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afa82af-f7d7-425a-8b5c-d95a824a5ab1",
   "metadata": {},
   "source": [
    "# Generating ImageNet Images\n",
    "\n",
    "Here we go through the steps to get a trained model to generate ImageNet images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495aa16a-481c-4831-860b-c66daffe5be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jo3/p/features2image_diffusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jo3/p/features2image_diffusion/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning:\n",
      "\n",
      "using dhist requires you to install the `pickleshare` library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80e84e-dec6-449b-a0c5-cbe8ff83f285",
   "metadata": {},
   "source": [
    "Our dataset, ImageNetSet will return features, images and labels, therefore we need to tell it the location of the features and the location of the stored images and labels.  \n",
    "We give an additional argument that gets passed to the torchvision.transform.Resize function, to reduce the size of the outputted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b6b1de-0c10-4c5d-a059-5ef6ded396ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee0e86423744523a6af789644fa5f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from features2image_diffusion.data import ImageNetSet\n",
    "\n",
    "dataset = ImageNetSet(\n",
    "    \"../sparse_autoencoder/run/1ae3fe65/train\",\n",
    "    \"../sparse_autoencoder/res/imagenet/train\",\n",
    "    image_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20db5fb9-d127-4327-acc7-6c223b86625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40768]) torch.Size([3, 64, 64]) <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40768, 3, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, image, label = dataset[0]\n",
    "print(features.shape, image.shape, type(label))\n",
    "\n",
    "num_features = features.shape[0]\n",
    "n_channels, _, img_len = image.shape\n",
    "num_features, n_channels, img_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d8daec-4c48-4031-8d1a-20595c7a0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from features2image_diffusion.unet import load_ddpm\n",
    "\n",
    "ddpm = load_ddpm(\n",
    "    \"run/f2a2e4ac/model/epoch-39.pth\",\n",
    "    n_classes=num_features,\n",
    "    n_channels=n_channels,\n",
    "    img_len=img_len,\n",
    "    hidden_size=128,\n",
    "    diffusion_steps=400,\n",
    "    device=\"cpu\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcc1a3f-0a64-476f-961c-65fe59572376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sampling timestep 400\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 256490078208 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     generations, _ \u001b[38;5;241m=\u001b[39m \u001b[48;5;53mddpm\u001b[49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53msample\u001b[49m\u001b[48;5;53m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[48;5;53m        \u001b[49m\u001b[48;5;53mcontext\u001b[49m\u001b[38;5;241;48;5;53m=\u001b[39;49m\u001b[48;5;53mTensor\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[48;5;53mfeatures\u001b[49m\u001b[48;5;53m)\u001b[49m\u001b[48;5;53m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[48;5;53m        \u001b[49m\u001b[48;5;53mn_sample\u001b[49m\u001b[38;5;241;48;5;53m=\u001b[39;49m\u001b[38;5;241;48;5;53m3\u001b[39;49m\u001b[48;5;53m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[48;5;53m        \u001b[49m\u001b[48;5;53msize\u001b[49m\u001b[38;5;241;48;5;53m=\u001b[39;49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m3\u001b[39;49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m64\u001b[39;49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m64\u001b[39;49m\u001b[48;5;53m)\u001b[49m\u001b[48;5;53m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[48;5;53m        \u001b[49m\u001b[48;5;53mdevice\u001b[49m\u001b[38;5;241;48;5;53m=\u001b[39;49m\u001b[38;5;124;48;5;53m\"\u001b[39;49m\u001b[38;5;124;48;5;53mcpu\u001b[39;49m\u001b[38;5;124;48;5;53m\"\u001b[39;49m\u001b[48;5;53m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[48;5;53m        \u001b[49m\u001b[48;5;53mverbose\u001b[49m\u001b[38;5;241;48;5;53m=\u001b[39;49m\u001b[38;5;28;48;5;53;01mTrue\u001b[39;49;00m\u001b[48;5;53m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[48;5;53m        \u001b[49m\u001b[48;5;53mstore\u001b[49m\u001b[38;5;241;48;5;53m=\u001b[39;49m\u001b[38;5;28;48;5;53;01mFalse\u001b[39;49;00m\u001b[48;5;53m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[48;5;53m    \u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m     14\u001b[0m generations\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/p/features2image_diffusion/features2image_diffusion/unet.py:330\u001b[0m, in \u001b[0;36mDDPM.sample\u001b[0;34m(self, context, n_sample, size, device, verbose, store)\u001b[0m\n\u001b[1;32m    327\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m*\u001b[39msize)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# split predictions and compute weighting\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mnn_model\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[48;5;53mx_i\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mc_i\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mt_is\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mcontext_mask\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# eps1 = eps[:batch_size]\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# eps2 = eps[batch_size:]\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# eps = (1 + guide_w) * eps1 - guide_w * eps2\u001b[39;00m\n\u001b[1;32m    334\u001b[0m x_i \u001b[38;5;241m=\u001b[39m x_i[:batch_size]\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53m_call_impl\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[48;5;53mforward_call\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/features2image_diffusion/unet.py:168\u001b[0m, in \u001b[0;36mContextUnet.forward\u001b[0;34m(self, x, c, t, context_mask)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c, t, context_mask):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# x is (noisy) image, c is context label, t is timestep,\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# context_mask says which samples to block the context on\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53minit_conv\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[48;5;53mx\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     down1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(x)\n\u001b[1;32m    170\u001b[0m     down2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(down1)\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53m_call_impl\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[48;5;53mforward_call\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/features2image_diffusion/unet.py:54\u001b[0m, in \u001b[0;36mResidualConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_res:\n\u001b[0;32m---> 54\u001b[0m         x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mconv1\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[48;5;53mx\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m     55\u001b[0m         x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x1)\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# this adds on correct residual in case channels have increased\u001b[39;00m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53m_call_impl\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[48;5;53mforward_call\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[48;5;53mmodule\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;28;48;5;53minput\u001b[39;49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53m_call_impl\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[48;5;53mforward_call\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53margs\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[38;5;241;48;5;53m*\u001b[39;49m\u001b[48;5;53mkwargs\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53m_conv_forward\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;28;48;5;53minput\u001b[39;49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mweight\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mbias\u001b[49m\u001b[48;5;53m)\u001b[49m\n",
      "File \u001b[0;32m~/p/features2image_diffusion/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[48;5;53mF\u001b[49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mconv2d\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;28;48;5;53minput\u001b[39;49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mweight\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mbias\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mstride\u001b[49m\u001b[48;5;53m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[48;5;53m                \u001b[49m\u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mpadding\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mdilation\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[38;5;241;48;5;53m.\u001b[39;49m\u001b[48;5;53mgroups\u001b[49m\u001b[48;5;53m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 256490078208 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    generations, _ = ddpm.sample(\n",
    "        context=Tensor(features),\n",
    "        n_sample=3,\n",
    "        size=(3, 64, 64),\n",
    "        device=\"cpu\",\n",
    "        verbose=True,\n",
    "        store=False,\n",
    "    )\n",
    "\n",
    "generations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b9c84-5185-4fc2-93be-bd0463c029c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22d4ba-9244-4c13-ae63-b67e442715e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jo3mnist.vis import to_img\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
